{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bandits Class\n",
    "\n",
    "<img src=\"figures/oab.jpg\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivating Example\n",
    "\n",
    "**Question**: Which route should you take to commute ?\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"figures/motiv1.png\"/></td>\n",
    "<td><img src=\"figures/motiv3.png\"/></td>\n",
    "<td><img src=\"figures/motiv2.png\"/></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Problem**: each day we obtain a _limited feedback_ (traveling time of the chosen route)\n",
    "- **Remarks**: repeatedly taking the same route \n",
    "    - increases our confidence on its true value...\n",
    "    - but potentially wastes your time !\n",
    "- **Strategy**: balance learning and optimization (aka Explore/Exploit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class overview\n",
    "- The problem\n",
    "    - difference with supervised learning\n",
    "- Applications\n",
    "- Performance metric\n",
    "- Classical Algorithms (stochastic bandits)\n",
    "    - E-greedy\n",
    "    - UCB\n",
    "    - Thompson Sampling\n",
    "- Extensions\n",
    "    - adversarial bandits\n",
    "    - contextual bandits\n",
    "- more generally, Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The problem\n",
    "\n",
    "## Difference with supervised learning\n",
    "\n",
    "- **supervised learning**: \n",
    "    - model $P(Y|X)$\n",
    "    - for each data point in the training set you have access to the true label\n",
    "- **bandit**: \n",
    "    - you have to *choose an action* (aka \"pull an arm\") to get feedback\n",
    "    - and you're not going to have feedback on unpulled arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications\n",
    "\n",
    "- Medecine\n",
    "    - test different variants of a drug with unknown power and side effects\n",
    "    - save as many patients as possible !\n",
    "- Packet routing\n",
    "    - choose different routes for a packet\n",
    "    - optimize for latency\n",
    "- News recommendations\n",
    "    - a single slot on homepage, many possible news\n",
    "    - optimize for clicks, dwell time etc\n",
    "- Online advertising\n",
    "    - an opportunity to display an ad, multiple ads available\n",
    "    - optimize for clicks, sales etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bandits at Criteo\n",
    "\n",
    "Each time a display is won, we can choose a specific creative.\n",
    "\n",
    "Suppose we come up with a list of products, how to choose the best creative ?\n",
    "\n",
    "The layout is composed in real time, choosing different parameters:\n",
    "- call to action\n",
    "- color set\n",
    "- interactivity\n",
    "- ...\n",
    "\n",
    "The search space is huge: $\\prod\\limits_{p}^{}{card(p)}$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"figures/darwin1.png\" style=\"width:150px\"/></td>\n",
    "<td><img src=\"figures/darwin3.png\" style=\"width:150px\"/></td>\n",
    "<td><img src=\"figures/darwin2.png\" style=\"width:150px\"/></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bandit Game\n",
    "\n",
    "- The learner has $i=1,\\ldots,K$ arms\n",
    "- At each round t = 1, . . . , n\n",
    "    - At the same time\n",
    "        - The environment chooses a vector of rewards $\\{X_{i,t}\\}$\n",
    "        - The learner chooses an arm $I_{t}$\n",
    "    - The learner receives a reward $X_{i,t}$\n",
    "    - The environment does not reveal the rewards of the other arms $X_{j,t}, j \\ne i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance metric: regret\n",
    "\n",
    "$$R_n(\\mathcal{A}) = \\max_{i=1,\\ldots,K}\\mathbf{E}[ \\sum_{t=1}^n{} X_{i,t} ] - \\mathbf{E}[ \\sum_{t=1}^n{} X_{I_t,t} ]$$\n",
    "\n",
    "where $\\mathbf{E}[.]$ takes into account any source of randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Basically, the expected reward difference between \"always pulling the best arm\" and \"pulling an arm according to algorithm $\\mathcal{A}$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since the environment doesn't give info on unpulled arms, the learner must pull many arms to gain confidence on $X_{i,}$ \n",
    "- But pulling a non-optimal arm induces regret...\n",
    "\n",
    "The learner should solve two contradictory objectives !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**<center>Explore vs Exploit</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regret (stochastic bandits)\n",
    "\n",
    "Suppose we are in a stochastic environment: rewards are i.id draws from probability distributions with a given mean $\\mu_i$.\n",
    "\n",
    "If we introduce $T_{i,n} = \\sum_{t=1}^{N} \\mathbf{1}[I_{t} = i]$ the number of pulls of arm $i$ at time $n$ we can rewrite regret as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R_n(\\mathcal{A}) = n \\mu_i^{*} - \\mathbf{E}[ \\sum_{t=1}^n{} X_{I_t,t} ] \\\\\n",
    "= \\sum_{i \\ne i^*} \\mathbf{E}[T_{i,n}](\\mu_i^* - \\mu_i) \n",
    "= \\sum_{i \\ne i^*} \\mathbf{E}[T_{i,n}]\\Delta_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With $\\Delta_i$ the mean difference between the best arm and the chosen arm $i$\n",
    "\n",
    "> in other words: we can focus on how many times on average the algo (aka policy) pulls non-optimal arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Remark**: if we had access to all rewards at time $t$, the optimal strategy would be to play $\\underset{i}{\\operatorname{argmax}} \\bar{X_{i,t}}$ where $\\bar{X_{i,t}} = \\frac{1}{n}\\sum_{t=1}^{n}X_{i,t}$\n",
    "\n",
    "Instead, we only have access to $\\hat{X_{i,t}} = \\frac{1}{|\\{I_t = i\\}|}\\sum_{t=1}^{n}X_{i,t}\\mathbf{1}[I_t = i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# E-Greedy\n",
    "\n",
    "- Algorithm:\n",
    "    - at each time step, choose randomly\n",
    "        - either an explore action, with probability $\\epsilon$\n",
    "        - or an exploit action with probability $1 - \\epsilon$\n",
    "    - explore = choose a random arm, uniformly\n",
    "    - exploit = choose best arm so far: $\\underset{i}{\\operatorname{argmax}} \\hat{X_{i,t}}$ \n",
    "\n",
    "> $\\epsilon$ controls the explore/exploit tradeoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example\n",
    "\n",
    "- 5 Bernoulli arms with $p = (.1, .1, .1, .1, .9)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/egreedy1.png\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# E-greedy analysis\n",
    "\n",
    "- simple, lower bound on regret:\n",
    "    - with proba at least $\\epsilon$, a suboptimal arm is chosen $(K-1)/K$ times\n",
    "    - so $\\mathbf{E_t}[R] > \\epsilon \\frac{K-1}{K} \\min_i{\\Delta_i}$\n",
    "\n",
    "- in our 5 Bernoulli arms example:\n",
    "    - lower bound on regret: $\\mathbf{E_t}[R] > \\epsilon \\times (4/5) \\times (.9 - .1) = .64 \\epsilon$\n",
    "    - how to choose $\\epsilon$ ? \n",
    "\n",
    "| $\\epsilon$ | strategy | $R>$ |\n",
    "|-----------|----------|--------------------------------|\n",
    "| .1 | exploit >> explore | .064 |\n",
    "| .5 | exploit = explore | .320 |\n",
    "\n",
    "Note: pay attention to the time horizon: if $max(t)=50$, $\\epsilon=.2$ is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond E-greedy\n",
    "\n",
    "- Weaknesses\n",
    "    - choosing the optimal $\\epsilon$ is non trivial (depends on horizon)\n",
    "    - after some time:\n",
    "        - we should have confidence on good vs bad arms\n",
    "        - but the exploration budget is still the same for both (does not depend on $\\hat{\\Delta_i}$)\n",
    "- Still, \n",
    "    - capable of adapting to non-stationary distributions at the cost of a constant exploration\n",
    "    - ultra-simple to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Improvement path:\n",
    "    - adapt the explore/exploit ratio over time\n",
    "    - $\\Rightarrow$ need to estimate the confidence of $\\hat{X_i,t}$ or $\\hat{\\Delta_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transition : estimation of means\n",
    "\n",
    "It appears that choosing the best arm boils down to estimating the mean reward of each arm $\\mu_i$ based on the experience we have pulling this arm $\\hat{X_{i,t}}$\n",
    "\n",
    "There are many statistics results one can use to derive bounds. \n",
    "\n",
    "A simple, non-parametric one is Hoeffding's concentration inequality:\n",
    "\n",
    "$$\\forall{t,i,\\epsilon > 0} \\quad P(\\hat{X_{i,t}} - \\mu_i > \\epsilon) < exp(-2t\\epsilon^2), $$\n",
    "\n",
    "> The probability to make an error of more than $\\epsilon$ decays exponentially with the number of samples $t$\n",
    "\n",
    "Corollary: the number of mistakes can be bounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UCB\n",
    "\n",
    "- So we can use H. inequality to build a confidence interval of $\\mu_i$ using $\\hat{X_{i,t}}$ and $|\\{I_t = i\\}|$\n",
    "\n",
    "- Strategy:\n",
    "    - instead of playing arm with maximum $\\hat{X_{i,t}}$ (which may be biased), play arm with maximum potential\n",
    "    - i.e. given a confidence interval on $\\hat{X_{i,t}}$, play the Upper Confidence Bound (UCB)\n",
    "\n",
    "<img src=\"figures/ucb_ci.png\" style=\"width:400px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UCB\n",
    "\n",
    "\n",
    "- Algorithm:\n",
    "    - play $\\underset{i}{\\operatorname{argmax}} \\hat{X_{i,t}} + \\sqrt{\\frac{2 log(t)}{|\\{I_t = i\\}|}}$ \n",
    "    - = f(knowledge + uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UCB regret analysis\n",
    "- H. inequality can be used to derive worst case performance: \n",
    "$$\\mathbf{E}[R_n] < O(\\frac{K log(n)}{\\Delta})$$\n",
    "    - with $\\Delta = \\underset{i}{\\operatorname{min}}(\\mu_i^* - \\mu_i)$\n",
    "    - $= f(\\Delta)$ $\\Rightarrow$ the algorithm adapts to the specific reward distribution\n",
    "\n",
    "\n",
    "\n",
    "- Worst regret possible ?\n",
    "    - $\\underset{\\Delta}{\\operatorname{sup}} R_n(\\Delta) \\Rightarrow \\Delta = 1/\\sqrt{n}$\n",
    "<img src=\"figures/ucb_worst.png\" style=\"width:450px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UCB regret analysis\n",
    "\n",
    "- compare with blindly playing $\\operatorname{argmax} \\hat{X_{i,t}}$: $\\mathbf{E}[R_n] < O(n)$\n",
    "\n",
    "> We now have a policy with logarithmic regret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ns = np.array([10**2, 10**3, 10**4, 10**5, 10**6, 10**7])\n",
    "deltas = 1/np.sqrt(ns)\n",
    "plt.semilogx(ns, deltas, label=\"worst delta\")\n",
    "plt.semilogx(ns, np.log(ns/deltas)/ns, label=\"$\\mathbf{E}[R]$\")\n",
    "plt.xlabel('n = #turns')\n",
    "#plt.ylabel('worse $\\Delta$')\n",
    "plt.title('Worst $\\Delta$ and regret for UCB')\n",
    "plt.legend()\n",
    "plt.savefig('figures/ucb_worst.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Extensions\n",
    "    - parametrized UCB when knowing the time horizon: knowledge + $\\rho$ uncertainty\n",
    "    - using parametric, yet tighter bounds (e.g. Bernstein, Martingale)\n",
    "    - regret bound as a function of the KL-divergence in proba. of $\\mu_i^*$ and $\\mu_i$\n",
    "    - Explore Then Commit policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond UCB\n",
    "\n",
    "- better practical performance ?\n",
    "- other learning paradigms ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thompson Sampling\n",
    "\n",
    "- Bayesian model\n",
    "    - true parameter $\\theta^*$ governing the reward process $P(r|I,\\theta^*)$\n",
    "    - ideally, play $\\underset{i}{\\operatorname{argmax}} \\mathbf{E}[r_i|I_i,\\theta^{*}]$\n",
    "- Estimation\n",
    "    - given, at any time $t$:\n",
    "        - past observations $D_t=\\{(I, r)\\}_{\\tau \\lt t}$\n",
    "        - the parametric likelihood function $P(r_t|I_t, \\theta)$\n",
    "        - a prior $P(\\theta)$\n",
    "    - we can compute with Bayes theorem\n",
    "        - posterior distribution: $P(\\theta|D_t) \\propto \\prod_t P(r_t|I_t,\\theta) P(\\theta)$\n",
    "- Strategy: probability matching heuristic\n",
    "    - choose action randomly according to its probabilty to be optimal\n",
    "        - draw $\\theta_t$ according to $P(\\theta|D_t)$\n",
    "        - observe reward, then update $P(\\theta|D_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TS for Bernoulli bandits\n",
    "\n",
    "- Model\n",
    "    - parameters: $\\theta=(S,F)$: success/failure counters\n",
    "    - realization probability: $P(r|\\theta) \\sim Beta(\\theta)$\n",
    "        - note that $\\mathbf{E}[Beta(S,F)] = \\frac{S}{S+F}$\n",
    "- Algorithm\n",
    "    - start with $S = F = 1$ (uniform prior)\n",
    "    - at each time step $t$\n",
    "        - draw $\\theta_t$ according to $P(r|\\theta)$\n",
    "        - pull arm $\\underset{i}{\\operatorname{argmax}}P(r_i|\\theta_t)$\n",
    "        - observe $r_{i,t}$, increment $S$ (if $r>0$) or $F$ (if $r=0$)\n",
    "        - update $P_t(r|\\theta) \\sim Beta(S_t, F_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/beta_update.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "x=np.linspace(0, 1, 1000)\n",
    "ax = plt.subplot()\n",
    "colors = cm.autumn(np.linspace(0, 1, 6))[::-1]\n",
    "#ax.plot(x, scipy.stats.beta.pdf(x, .5, .5), label='$\\Theta=(.5,.5$)', color=colors[0])\n",
    "ax.plot(x, scipy.stats.beta.pdf(x, 1, 1), label='$\\Theta=(1,1$)', color=colors[1])\n",
    "ax.plot(x, scipy.stats.beta.pdf(x, 1, 10), label='$\\Theta=(1,10$)', color=colors[2])\n",
    "ax.plot(x, scipy.stats.beta.pdf(x, 10, 100), label='$\\Theta=(10,100$)', color=colors[3])\n",
    "ax.plot(x, scipy.stats.beta.pdf(x, 100, 1000), label='$\\Theta=(100,1000$)', color=colors[4])\n",
    "ax.plot([.1, .1],[0, 50],'g--', label='$\\Theta^*$')\n",
    "plt.legend()\n",
    "plt.title('Beta model update for $\\Theta^* = .1$')\n",
    "plt.savefig('figures/beta_update.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison: TS vs UCB\n",
    "- Synthetic data generated by K Bernoulli arms with $\\theta^* = (.5+\\epsilon, .5, \\ldots, .5)$\n",
    "<img src=\"figures/thompsonvsucb.png\" style=\"width:800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's next ?\n",
    "\n",
    "## Adversarial bandits\n",
    "- adversarial environment \n",
    "    - e.g. spam filter: defender can react to attacker strategy (learn to filter specific spams)\n",
    "- rewards are no more i.i.d., but chosen by the environment\n",
    "    \n",
    "$$R_n(\\mathcal{A}) = \\max_{i=1,\\ldots,K}\\sum_{t=1}^n{} X_{i,t} - \\sum_{t=1}^n{} X_{I_t,t}$$\n",
    "\n",
    "- The benchmark is still the best arm but it's not stochastic anymore\n",
    "- Goal: achieve decent (sub-linear) regret vs any adversary\n",
    "    - in particular against any $X_{i,t}$ sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXP3\n",
    "\n",
    "- Exponential-weight algorithm for Exploration and Exploitation\n",
    "- Algorithm\n",
    "    - parameter: $\\eta$ (learning rate)\n",
    "    - initialize arm weights $w_{i,0} = 1$\n",
    "    - at each time step $t$\n",
    "        - compute $W_{t-1} = \\sum\\limits_{i=1}^K w_{i, t-1}$ *(a normalizing constant)*\n",
    "        - compute $\\hat{p}_{i,t} = \\frac{w_{i, t-1}}{W_{t-1}}$ *(proba. to choose action)*\n",
    "        - draw an arm randomly according to $\\hat{p}_{i,y}$\n",
    "        - estimate importance of reward: $\\tilde{X}_{i,t} = \\frac{X_{i,t}}{{t\\cdot\\hat{p}_{i}}}$ \n",
    "            - reward, weighted by proba. to choose action (importance sampling estimate, unbiased)\n",
    "            - i.e. the reward is weighted by its surprise potential\n",
    "        - update $w_{i, t} = w_{i, t-1} exp(\\eta \\tilde{X}_{i,t})$\n",
    "            - i.e. exponential update rule\n",
    "\n",
    "\n",
    "- Note: in practice add an ounce of extra exploration... (inc. another parameter to optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation\n",
    "\n",
    "<img src=\"figures/exp3_run.png\" style=\"width:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EXP3 regret analysis\n",
    "\n",
    "- in short\n",
    "    - $\\mathbf{E}[R_n] \\le O(\\sqrt{n K log K})$\n",
    "- in comparison, online learning with full monitoring achieves\n",
    "    - $\\mathbf{E}[R_n] \\le O(\\sqrt{n log K})$\n",
    "    - we loose a factor of $K$ due to bandit monitoring (1 vs $K$ feedbacks at each turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's next (cont'd) ?\n",
    "\n",
    "## Contextual bandits\n",
    "\n",
    "- You have access to additional information: a context, at the beginning of every turn\n",
    "    - closer to reality (e.g. user variables in case of ad selection)\n",
    "    - You should now act according to $P(r_{i,t}|I_{i,t}, c_t)$\n",
    "    \n",
    "- Remark\n",
    "    - context-aware policies have potentially more to learn: $\\sim O(K \\times |C|)$\n",
    "    - a simple solution is to learn a bandit per context\n",
    "    - $\\Rightarrow$ needs much more exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Solution path\n",
    "    - possible strategies\n",
    "        - reduce dimensionality of contexts (back to small nber of arms scenario)\n",
    "        - learn a (simple) model of reward as a function of context, e.g. $r(a_i,c_i) = <(a_i,c_i) ; \\alpha^*>$\n",
    "    - algorithms in the spirit of what we've seen have been proposed\n",
    "        - [Contextual Thompson Sampling](http://www.jmlr.org/proceedings/papers/v28/agrawal13.pdf)\n",
    "        - [EXP4](http://www.jmlr.org/proceedings/papers/v15/beygelzimer11a/beygelzimer11a.pdf)\n",
    "        - [LinUCB](http://www.jmlr.org/proceedings/papers/v15/chu11a/chu11a.pdf)\n",
    "- Still an active research area "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Link with reinforcement learning\n",
    "\n",
    "\n",
    "- Imagine a bandit with a state...\n",
    "    - e.g. in the packet routing case\n",
    "        - learner chooses which route to send packet to destination\n",
    "        - depending on chosen route there is a chance that next turn you gain/loose access to different routes\n",
    "    - e.g. in the ad selection case\n",
    "        - depending on chosen ad\n",
    "            - new ads are available / old ads disappear\n",
    "            - users change behavior (hence the reward function changes)\n",
    "\n",
    "Overall, a much richer/powerfull/challenging learning problem\n",
    "\n",
    "> A bandit is merely a Markov Decision Process with 1 state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take aways\n",
    "\n",
    "- Learning *with limited feedback*\n",
    "- Tradeoff information vs performance\n",
    "\n",
    "\n",
    "- Available policies for\n",
    "    - stochastic: log(n) regret\n",
    "    - adverserial: sqrt(n) regret\n",
    "\n",
    "\n",
    "- Links with online/reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "## Material used in the class\n",
    "This class borrows material from the following resources:\n",
    "\n",
    "- Courses\n",
    "    - [A. Lazaric: Advanced topics in ML (slides)](http://researchers.lille.inria.fr/~lazaric/Webpage/Home/Entries/2012/1/31_Course_on_%22Advanced_topics_of_machine_learning_theory_and_online_learning%22_files/poli-bandit.pdf)\n",
    "    - [S. Katariya: Bandits intro (slides)](http://homepages.cae.wisc.edu/~sumeet/files/banditsslides.pdf)\n",
    "    - [V. Perchet: (course + slides)](https://sites.google.com/site/vianneyperchet/)\n",
    "- Papers\n",
    "    - [Chapelle et al., An Empirical Evaluation of Thompson Sampling](https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling)\n",
    "    - [Auer et al., UCB revisited](http://personal.unileoben.ac.at/rortner/Pubs/UCBRev.pdf)\n",
    "- Blogs\n",
    "    - [Kun: EXP3 proof](https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/)\n",
    "    - [Langford: Hunch.net](http://hunch.net/?p=298)\n",
    "    - [Szepesvari: Banditalgs](http://banditalgs.com/) a nice blog that'll end up as a book\n",
    "    - [Bubeck: Bandits state of the art](https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/)\n",
    "\n",
    "## Additional readings\n",
    "\n",
    "If interested, I encourage you to read\n",
    "- Olivier's paper (for empirical evaluation & applications) \n",
    "- Vianney's course (for detail and spirit of the proofs)\n",
    "\n",
    "and perhaps to subscribe to Szepesvari's blog :)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
